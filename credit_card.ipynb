{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kredi KartÄ± DolandÄ±rÄ±cÄ±lÄ±ÄŸÄ± Tespiti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ã–nceki egzersizler, bir sinir aÄŸÄ±nÄ±n tÃ¼m farklÄ± bileÅŸenlerine daha yakÄ±ndan bakmanÄ±zÄ± saÄŸladÄ±:\n",
    "* sÄ±ralÄ± (sequential) yoÄŸun (Dense) bir Sinir AÄŸÄ± mimarisi,\n",
    "* derleme (compilation) yÃ¶ntemi,\n",
    "* modelin eÄŸitilmesi (fitting).\n",
    "\n",
    "Åimdi **Ã§ok fazla veri iÃ§eren** gerÃ§ek hayat bir veri seti Ã¼zerinde Ã§alÄ±ÅŸalÄ±m!\n",
    "\n",
    "**Veri seti: `Kredi KartÄ± Ä°ÅŸlemleri (Credit Card Transactions)`**\n",
    "\n",
    "Bu aÃ§Ä±k uÃ§lu challengeâ€™da, **kredi kartÄ± iÅŸlemlerinden elde edilmiÅŸ verilerle** Ã§alÄ±ÅŸacaksÄ±nÄ±z.\n",
    "\n",
    "Bu veriler **hassas** olduÄŸu iÃ§in, toplam 31 sÃ¼tundan yalnÄ±zca 3â€™Ã¼ bilinmektedir; geri kalanlar verileri **anonimleÅŸtirmek** amacÄ±yla dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r (aslÄ±nda bunlar, orijinal verilerin **PCA projeksiyonlarÄ±dÄ±r**).\n",
    "\n",
    "Bilinen 3 sÃ¼tun ÅŸunlardÄ±r:\n",
    "\n",
    "* `TIME`: Ä°ÅŸlemin, veri setindeki ilk iÅŸleme gÃ¶re geÃ§en sÃ¼resi  \n",
    "* `AMOUNT`: Ä°ÅŸlem tutarÄ±  \n",
    "* `CLASS` (hedef deÄŸiÅŸkenimiz):\n",
    "    * `0 : geÃ§erli iÅŸlem`\n",
    "    * `1 : sahte (fraud) iÅŸlem`\n",
    "\n",
    "â“ **Soru** â“ Veri setini indirerek baÅŸlayÄ±n:\n",
    "* Kaggle Ã¼zerinden: [buradan](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n",
    "* veya bizim URLâ€™mizden: [buradan](https://d32aokrjazspmn.cloudfront.net/materials/creditcard.csv)\n",
    "\n",
    "Veriyi yÃ¼kleyerek `X` ve `y` deÄŸiÅŸkenlerini oluÅŸturun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X boyutu: (284807, 30)\n",
      "y boyutu: (284807,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Veriyi verilen linkten doÄŸrudan Ã§ekiyoruz\n",
    "url = \"https://d32aokrjazspmn.cloudfront.net/materials/creditcard.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Veriyi X ve y olarak ayÄ±ralÄ±m\n",
    "X = df.drop(columns=['Class'])  # Cevap anahtarÄ±nÄ± (Class) Ã§Ä±kar, geriye kalanlar ipuÃ§larÄ±dÄ±r\n",
    "y = df['Class']                 # Sadece cevap anahtarÄ±nÄ± al\n",
    "\n",
    "# Verinin boyutlarÄ±na bakalÄ±m (SatÄ±r, SÃ¼tun)\n",
    "print(f\"X boyutu: {X.shape}\")\n",
    "print(f\"y boyutu: {y.shape}\")# SENÄ°N KODUN BURAYA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SÄ±nÄ±flarÄ±n yeniden dengelenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    0.998273\n",
       "1    0.001727\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SÄ±nÄ±f dengesini kontrol edelim\n",
    "pd.Series(y).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â˜ï¸ Bu `fraud detection` (sahtekÃ¢rlÄ±k tespiti) challengeâ€™Ä±nda **sÄ±nÄ±flar aÅŸÄ±rÄ± derecede dengesizdir**:\n",
    "* %99.8 normal iÅŸlemler\n",
    "* %0.2 sahte (fraud) iÅŸlemler\n",
    "\n",
    "**Ciddi yeniden dengeleme (rebalancing) stratejileri uygulamazsak sahtekÃ¢rlÄ±k vakalarÄ±nÄ± tespit edemeyiz!**\n",
    "\n",
    "â“ **Soru** â“\n",
    "1. **Ã–nce**, veri setinizden Ã¼Ã§ ayrÄ± bÃ¶lme oluÅŸturun: `Train / Val / Test`.  \n",
    "   DoÄŸrulama (validation) ve test setlerinin **dengesiz** kalmasÄ± son derece Ã¶nemlidir; bÃ¶ylece modeli deÄŸerlendirirken gerÃ§ek koÅŸullar korunur ve veri sÄ±zÄ±ntÄ±sÄ± (data leakage) oluÅŸmaz. Test setinizi bu notebookâ€™un **en son hÃ¼cresine kadar saklayÄ±n**!\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "2. **Ä°kinci olarak**, yalnÄ±zca eÄŸitim setinizi (training set) yeniden dengeleyin. BirÃ§ok seÃ§eneÄŸiniz var:\n",
    "\n",
    "- AzÄ±nlÄ±k sÄ±nÄ±fÄ±nÄ± rastgele oversample etmek (dÃ¼z NumPy fonksiyonlarÄ±yla).  \n",
    "  *(En iyi seÃ§enek deÄŸildir; Ã§Ã¼nkÃ¼ satÄ±rlarÄ± kopyalayarak veri sÄ±zÄ±ntÄ±sÄ± yaratÄ±r.)*\n",
    "- <a href=\"https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\">**`Synthetic Minority Oversampling Technique - SMOTE`**</a> kullanarak, mevcut gÃ¶zlemleri aÄŸÄ±rlÄ±klandÄ±rÄ±p yeni veri noktalarÄ± Ã¼retmek\n",
    "- Buna ek olarak, Ã§oÄŸunluk sÄ±nÄ±fÄ±nÄ± bir miktar kÃ¼Ã§Ã¼ltmek iÃ§in  \n",
    "  <a href=\"https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/\">**`RandomUnderSampler`**</a> da deneyebilirsiniz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orijinal Train seti boyutu: (139554, 30)\n",
      "Orijinal Train iÃ§indeki dolandÄ±rÄ±cÄ± oranÄ±: 0.0017\n",
      "\n",
      "--- SMOTE SonrasÄ± ---\n",
      "Yeni Train seti boyutu: (278626, 30)\n",
      "Yeni Train iÃ§indeki dolandÄ±rÄ±cÄ± sayÄ±sÄ±: 139313\n",
      "Yeni Train iÃ§indeki normal iÅŸlem sayÄ±sÄ±: 139313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 1. ADIM: Veriyi AyÄ±rma (Train / Val / Test)\n",
    "# Ã–nce Test setini ayÄ±ralÄ±m (Kenara koyuyoruz, dokunmayacaÄŸÄ±z)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Åimdi elimizde kalan parÃ§ayÄ± Train ve Validation olarak ayÄ±ralÄ±m\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.3, random_state=42, stratify=y_train_val)\n",
    "\n",
    "# Kontrol edelim: Åu an her ÅŸey hala orjinal ve dengesiz mi?\n",
    "print(f\"Orijinal Train seti boyutu: {X_train.shape}\")\n",
    "print(f\"Orijinal Train iÃ§indeki dolandÄ±rÄ±cÄ± oranÄ±: {sum(y_train)/len(y_train):.4f}\")\n",
    "\n",
    "\n",
    "# 2. ADIM: SADECE EÄŸitim Setini Dengeleme (SMOTE)\n",
    "# Teraziyi dengeliyoruz: AzÄ±nlÄ±k sÄ±nÄ±fÄ± (dolandÄ±rÄ±cÄ±larÄ±) sentetik olarak Ã§oÄŸaltÄ±yoruz.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# SonuÃ§larÄ± gÃ¶relim\n",
    "print(\"\\n--- SMOTE SonrasÄ± ---\")\n",
    "print(f\"Yeni Train seti boyutu: {X_train_smote.shape}\")\n",
    "print(f\"Yeni Train iÃ§indeki dolandÄ±rÄ±cÄ± sayÄ±sÄ±: {sum(y_train_smote)}\")\n",
    "print(f\"Yeni Train iÃ§indeki normal iÅŸlem sayÄ±sÄ±: {len(y_train_smote) - sum(y_train_smote)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sinir AÄŸÄ± yinelemeleri\n",
    "\n",
    "SÄ±nÄ±flarÄ±nÄ±zÄ± yeniden dengelediÄŸinize gÃ¶re, test puanÄ±nÄ±zÄ± optimize etmek iÃ§in bir sinir aÄŸÄ± uyarlayÄ±n. AÅŸaÄŸÄ±daki ipuÃ§larÄ±nÄ± kullanmaktan Ã§ekinmeyin:\n",
    "\n",
    "- GiriÅŸlerinizi normalleÅŸtirin!\n",
    "    - Modelinizdeki Ã¶n iÅŸlemeyi â€œboru hattÄ±â€ haline getirmek iÃ§in tercihen model iÃ§inde bir [`Normalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization) katmanÄ± kullanÄ±n. \n",
    "    - Veya modelinizin dÄ±ÅŸÄ±nda sklearn'in [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) Ã¶ÄŸesini kullanÄ±n, `X_train`, `X_val` ve `X_test` Ã¶ÄŸelerini uygulayÄ±n.\n",
    "- Modelinizi aÅŸÄ±rÄ± uyumlu hale getirin, ardÄ±ndan aÅŸaÄŸÄ±dakileri kullanarak dÃ¼zenleyin:\n",
    "- Erken Durdurma kriterleri\n",
    "- [`Dropout`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) katmanlarÄ±\n",
    "    - veya [`dÃ¼zenleyiciler`](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers) katmanlarÄ±\n",
    "- ğŸš¨ Ä°zlemek istediÄŸiniz metrikleri ve kullanmak istediÄŸiniz kayÄ±p fonksiyonunu dikkatlice dÃ¼ÅŸÃ¼nÃ¼n!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 23:50:16.688999: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-21 23:50:16.701757: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-21 23:50:16.800841: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-21 23:50:16.929915: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-21 23:50:17.045373: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-21 23:50:17.046120: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-21 23:50:17.223940: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-21 23:50:18.493922: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tumay/.pyenv/versions/workintech/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2026-01-21 23:50:20.427221: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-01-21 23:50:20.429001: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9544 - loss: 0.1290 - precision: 0.9655 - recall: 0.9425 - val_accuracy: 0.9925 - val_loss: 0.0314 - val_precision: 0.1770 - val_recall: 0.9126\n",
      "Epoch 2/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9820 - loss: 0.0556 - precision: 0.9836 - recall: 0.9804 - val_accuracy: 0.9952 - val_loss: 0.0233 - val_precision: 0.2520 - val_recall: 0.9126\n",
      "Epoch 3/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9862 - loss: 0.0424 - precision: 0.9851 - recall: 0.9873 - val_accuracy: 0.9963 - val_loss: 0.0191 - val_precision: 0.3056 - val_recall: 0.8932\n",
      "Epoch 4/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9880 - loss: 0.0369 - precision: 0.9859 - recall: 0.9901 - val_accuracy: 0.9962 - val_loss: 0.0188 - val_precision: 0.2987 - val_recall: 0.8932\n",
      "Epoch 5/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9885 - loss: 0.0353 - precision: 0.9859 - recall: 0.9911 - val_accuracy: 0.9966 - val_loss: 0.0188 - val_precision: 0.3229 - val_recall: 0.9029\n",
      "Epoch 6/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9888 - loss: 0.0337 - precision: 0.9862 - recall: 0.9914 - val_accuracy: 0.9960 - val_loss: 0.0205 - val_precision: 0.2906 - val_recall: 0.9029\n",
      "Epoch 7/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9899 - loss: 0.0316 - precision: 0.9875 - recall: 0.9923 - val_accuracy: 0.9959 - val_loss: 0.0216 - val_precision: 0.2840 - val_recall: 0.9126\n",
      "Epoch 8/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9896 - loss: 0.0322 - precision: 0.9866 - recall: 0.9926 - val_accuracy: 0.9977 - val_loss: 0.0176 - val_precision: 0.4247 - val_recall: 0.9029\n",
      "Epoch 9/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9900 - loss: 0.0308 - precision: 0.9871 - recall: 0.9929 - val_accuracy: 0.9966 - val_loss: 0.0195 - val_precision: 0.3229 - val_recall: 0.9029\n",
      "Epoch 10/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9901 - loss: 0.0303 - precision: 0.9871 - recall: 0.9932 - val_accuracy: 0.9968 - val_loss: 0.0205 - val_precision: 0.3370 - val_recall: 0.8835\n",
      "Epoch 11/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9903 - loss: 0.0307 - precision: 0.9871 - recall: 0.9935 - val_accuracy: 0.9965 - val_loss: 0.0209 - val_precision: 0.3153 - val_recall: 0.9029\n",
      "Epoch 12/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9906 - loss: 0.0295 - precision: 0.9874 - recall: 0.9938 - val_accuracy: 0.9963 - val_loss: 0.0222 - val_precision: 0.3054 - val_recall: 0.8835\n",
      "Epoch 13/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9907 - loss: 0.0294 - precision: 0.9874 - recall: 0.9940 - val_accuracy: 0.9976 - val_loss: 0.0197 - val_precision: 0.4152 - val_recall: 0.9029\n",
      "Epoch 14/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9905 - loss: 0.0292 - precision: 0.9871 - recall: 0.9940 - val_accuracy: 0.9966 - val_loss: 0.0216 - val_precision: 0.3204 - val_recall: 0.8835\n",
      "Epoch 15/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9910 - loss: 0.0287 - precision: 0.9877 - recall: 0.9945 - val_accuracy: 0.9965 - val_loss: 0.0222 - val_precision: 0.3140 - val_recall: 0.8932\n",
      "Epoch 16/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9911 - loss: 0.0282 - precision: 0.9879 - recall: 0.9943 - val_accuracy: 0.9963 - val_loss: 0.0233 - val_precision: 0.3033 - val_recall: 0.8835\n",
      "Epoch 17/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9911 - loss: 0.0276 - precision: 0.9879 - recall: 0.9944 - val_accuracy: 0.9973 - val_loss: 0.0233 - val_precision: 0.3734 - val_recall: 0.8738\n",
      "Epoch 18/50\n",
      "\u001b[1m4354/4354\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9912 - loss: 0.0278 - precision: 0.9882 - recall: 0.9943 - val_accuracy: 0.9969 - val_loss: 0.0222 - val_precision: 0.3434 - val_recall: 0.8835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import models, layers, metrics, callbacks\n",
    "\n",
    "# --- 1. NORMALÄ°ZASYON (SCALING) ---\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scaler'Ä± sadece EÄÄ°TÄ°M setine gÃ¶re ayarla (fit)\n",
    "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "\n",
    "# AynÄ± ayarÄ± Val ve Test setlerine uygula (transform)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- 2. MODEL MÄ°MARÄ°SÄ° ---\n",
    "model = models.Sequential()\n",
    "\n",
    "# GiriÅŸ KatmanÄ± + Ä°lk Gizli Katman\n",
    "# input_dim = SÃ¼tun sayÄ±sÄ± (30 civarÄ±)\n",
    "model.add(layers.Dense(32, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
    "model.add(layers.Dropout(0.5)) # NÃ¶ronlarÄ±n %50'sini rastgele kapat (Ezberlemeyi Ã¶nler)\n",
    "\n",
    "# Ä°kinci Gizli Katman\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Ã‡Ä±kÄ±ÅŸ KatmanÄ±\n",
    "# DolandÄ±rÄ±cÄ± mÄ± (1) deÄŸil mi (0)? Tek bir nÃ¶ron ve Sigmoid yeterli.\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# --- 3. DERLEME (COMPILE) ---\n",
    "# Metrics kÄ±smÄ±na dikkat! Sadece accuracy deÄŸil, Recall ve Precision da ekliyoruz.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', metrics.Recall(name='recall'), metrics.Precision(name='precision')])\n",
    "\n",
    "# --- 4. EÄÄ°TÄ°M (FIT) ---\n",
    "# EarlyStopping: EÄŸer 'val_loss' 10 epoch boyunca iyileÅŸmezse eÄŸitimi durdur.\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train_smote,\n",
    "                    validation_data=(X_val_scaled, y_val),\n",
    "                    batch_size=64, # Her seferde 64 veri al\n",
    "                    epochs=50,     # En fazla 50 tur dÃ¶n (EarlyStopping muhtemelen Ã¶nce durdurur)\n",
    "                    callbacks=[es],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª Kodunu Test Et"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orijinal dengesiz veri kÃ¼mesinin (`X_test`, `y_test`) temsil edici bir Ã¶rneÄŸinde gerÃ§ek test performansÄ±nÄ±zÄ±n altÄ±nda kalan sonuÃ§larÄ± `precision` ve `recall` deÄŸiÅŸkenlerine kaydedin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2671/2671\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 507us/step\n",
      "Precision (Kesinlik): 0.3708\n",
      "Recall (DuyarlÄ±lÄ±k): 0.8243\n",
      "\n",
      "--- Yorum ---\n",
      "Tespit edilen dolandÄ±rÄ±cÄ± oranÄ± (Recall): %82.43\n",
      "SuÃ§ladÄ±klarÄ±mÄ±zdan gerÃ§ekten dolandÄ±rÄ±cÄ± olanlar (Precision): %37.08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# 1. Modelden tahminleri al (OlasÄ±lÄ±k olarak dÃ¶ner: Ã¶rn 0.85, 0.12)\n",
    "y_pred_probs = model.predict(X_test_scaled)\n",
    "\n",
    "# 2. OlasÄ±lÄ±klarÄ± 0 veya 1'e Ã§evir (EÅŸik deÄŸerimiz 0.5)\n",
    "# %50'den bÃ¼yÃ¼kse DolandÄ±rÄ±cÄ± (1), kÃ¼Ã§Ã¼kse Normal (0) diyelim\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# 3. SkorlarÄ± hesapla\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision (Kesinlik): {precision:.4f}\")\n",
    "print(f\"Recall (DuyarlÄ±lÄ±k): {recall:.4f}\")\n",
    "\n",
    "print(\"\\n--- Yorum ---\")\n",
    "print(f\"Tespit edilen dolandÄ±rÄ±cÄ± oranÄ± (Recall): %{recall*100:.2f}\")\n",
    "print(f\"SuÃ§ladÄ±klarÄ±mÄ±zdan gerÃ§ekten dolandÄ±rÄ±cÄ± olanlar (Precision): %{precision*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.9, pytest-8.3.4, pluggy-1.5.0 -- /home/tumay/.pyenv/versions/workintech/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/tumay/code/S18D3-S-Data-credit-card-challenge/tests\n",
      "plugins: anyio-4.8.0, dash-3.3.0, typeguard-4.4.2\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_solution.py::TestSolution::test_is_score_good_enough \u001b[32mPASSED\u001b[0m\u001b[32m         [ 50%]\u001b[0m\n",
      "test_solution.py::TestSolution::test_is_test_set_representative \u001b[32mPASSED\u001b[0m\u001b[32m   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "ğŸ’¯ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/solution.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed solution step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('solution',\n",
    "    precision=precision,\n",
    "    recall=recall,\n",
    "    fraud_number=len(y_test[y_test == 1]),\n",
    "    non_fraud_number=len(y_test[y_test == 0]),\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ Ä°steÄŸe baÄŸlÄ±: Bu zorluk iÃ§in Google'Ä±n Ã§Ã¶zÃ¼mÃ¼nÃ¼ okuyun\n",
    "Bu oturumdaki tÃ¼m zorluklarÄ± tamamladÄ±ÄŸÄ±nÄ±z iÃ§in tebrikler!\n",
    "\n",
    "Son olarak, Google'Ä±n kendi Ã§Ã¶zÃ¼mÃ¼nÃ¼ doÄŸrudan [Colab'da buradan](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb) okuyun. \n",
    "\n",
    "Ä°lginÃ§ teknikler ve en iyi uygulamalarÄ± keÅŸfedeceksiniz."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workintech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
